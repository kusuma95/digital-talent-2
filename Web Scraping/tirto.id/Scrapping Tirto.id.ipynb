{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Website tirto.id \n",
    "---\n",
    "by: __Bakhtiar Amaludin__ | Digital Talent 2018 @ Univ. Gadjah Mada | maziyank@gmail.com | @maziyank\n",
    "***\n",
    "\n",
    "Notebook ini adalah hasil dari kegiatan latihan _web scraping_ berita yang ada dalam website tirto.id. _Scraping_ dilakukan melalui halaman indeks berita dengan pembatasan pada jumlah berita terakhit. Berikut adalah beberapa informasi umum terkait _web scraping_ ini:\n",
    "\n",
    "- Portal Berita (sumber) : tirto.id (URL: https://tirto.id/indeks/)\n",
    "- Parameter              : Limit Latest News\n",
    "- Data yang diambil      : Judul, Isi, Tanggal, Author, Gambar\n",
    "- Format _output_        : .json\n",
    "- Penyimpanan Image      : ./images/\n",
    "\n",
    "_libraries_ yang digunakan:\n",
    "- urllib                 : untuk mengambil data dari internet\n",
    "- BeautifulSoup          : untuk melakukan parsing dan ekstraksi data halaman web\n",
    "- datetime               : untuk melakukan rutin-rutin yang berhubungan dengan tanggal\n",
    "- json                   : untuk menyimpan output ke dalam file .json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Inisiasi libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "hdr = { 'User-Agent': 'My User Agent 2.0',\n",
    "        'From': 'youremails@domain.com'  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Membuat Fungsi Untuk Mendapatkan URL Berita dari Indeks\n",
    "---\n",
    "Sebelum melakukan ekstraksi data berita, terlebih dahulu dilakukan pengumpulan links dari halaman indeks pada website yang akan discrap. Nah, fungsi ini digunakan untuk mendapatkan semua url dari berita yang ada dalam halaman indeks tersebut. Kebetulan website tirto ini sudah memiliki api untuk menampilkan list seluruh berita terakhir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinks(limit):    \n",
    "    url = 'https://tirto.id/apis/articlef/latest/all?page=1&limit=' + str(limit)    \n",
    "\n",
    "    request = urllib.request.Request(url, headers=hdr)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    \n",
    "    data = json.loads(response.read())    \n",
    "    with open('tirto_links.json', 'w') as outfile:  \n",
    "        json.dump(data, outfile)        \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Membuat Fungsi Untuk Membaca Berita\n",
    "\n",
    "Dari _link single page_ berita yang telah diperoleh, langkah berikutnya adalah mengekstrasi data dari halaman yang diperoleh dari _link_ tersebut. Fungsi ini dibuat untuk melakukan tugas tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBerita(sublink):\n",
    "    url = 'https://tirto.id' + str(sublink)    \n",
    "    \n",
    "    request = urllib.request.Request(url, headers=hdr)\n",
    "    response = urllib.request.urlopen(request)    \n",
    "    \n",
    "    soup = BeautifulSoup(response.read(),\"html.parser\")      \n",
    "\n",
    "    content = json.loads(soup.find_all('script')[6].text.replace('window.__NUXT__=','')[:-1])['data'][0]['article']\n",
    "    \n",
    "     # mengambil judul dari artikel     \n",
    "    judul = content['judul']\n",
    "\n",
    "    # mengambil tanggal dari artikel     \n",
    "    tanggal = content['date_news']\n",
    "    \n",
    "    # membersikan isi berita\n",
    "    isi = content['isi']\n",
    "\n",
    "    # mengambil url gambar dari artikel     \n",
    "    gambar = 'https://mmc.tirto.id/image/otf/700x0/' + content['image'][0]['url']\n",
    "\n",
    "    # mengambil nama redaksi dari artikel     \n",
    "    author = content['author'][0]['nama_author']\n",
    "    \n",
    "    # simpan image berita ke folder images\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    urllib.request.install_opener(opener)\n",
    "    urllib.request.urlretrieve(gambar, \"images/\"+content['image'][0]['nama_file'])    \n",
    "            \n",
    "    return {\"judul\": judul, \"isi\": isi, \"tanggal\":tanggal, \"author\":author, \"gambar\": gambar }    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Mulai Scraping \n",
    "\n",
    "Proses _web scraping_ dilakukan pada tahap ini menggunakan dua fungsi yang telah dibuat sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping : /winamp-yang-mati-kini-terlahir-kembali-c8nZ\n",
      "scraping : /karnaval-mandiri-apresiasi-kinerja-karyawan-c8H6\n",
      "scraping : /hasil-french-open-2018-greysia-apriyani-disingkirkan-ganda-jepang-c8H1\n",
      "scraping : /jelang-timnas-u-19-indonesia-vs-jepang-egy-todd-siap-tampil-c8H5\n",
      "scraping : /rembuk-dan-hubungannya-dengan-revolusi-mental-c8H4\n",
      "Scraping Done.. \n"
     ]
    }
   ],
   "source": [
    "berita = []\n",
    "limit = 1\n",
    "\n",
    "# mendaptkan seluruh links sesuai parameter seberlumnya\n",
    "links = getLinks(5)\n",
    "\n",
    "# ekstraksi data dari setiap link yang telah didapat dan menyimpanya ke variabel berita[]\n",
    "for link in links['data']:        \n",
    "    getBerita(link['articleUrl'])\n",
    "    berita.append(getBerita(link['articleUrl']))\n",
    "    print('scraping :', link['articleUrl'])\n",
    "      \n",
    "# menyimpan hasil scraping ke dalam file\n",
    "with open('tirto_contents.json', 'w') as outfile:  \n",
    "    json.dump(berita, outfile)\n",
    "    print('Scraping Done.. ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
