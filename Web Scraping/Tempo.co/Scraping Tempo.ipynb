{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Website Tempo.co \n",
    "---\n",
    "by: __Bakhtiar Amaludin__ | Digital Talent 2018 @ Univ. Gadjah Mada | maziyank@gmail.com | @maziyank\n",
    "***\n",
    "\n",
    "Notebook ini adalah hasil dari kegiatan latihan _web scraping_ berita yang ada dalam website tempo.co. _Scraping_ dilakukan melalui halaman indeks berita dengan pembatasan pada tanggal dan kategori berita. Berikut adalah beberapa informasi umum terkait _web scraping_ ini:\n",
    "\n",
    "- Portal Berita (sumber) : tempo.co (URL: https://www.tempo.co/indeks/)\n",
    "- Parameter              : Kategori dan Range Tanggal\n",
    "- Data yang diambil      : Judul, Isi, Tanggal, Author, Editor, Tags, Gambar\n",
    "- Format _output_        : .json\n",
    "- Penyimpanan Image      : ./images/\n",
    "\n",
    "_libraries_ yang digunakan:\n",
    "- urllib                 : untuk mengambil data dari internet\n",
    "- StringIO               : untuk menulis file\n",
    "- BeautifulSoup          : untuk melakukan parsing dan ekstraksi data halaman web\n",
    "- datetime               : untuk melakukan rutin-rutin yang berhubungan dengan tanggal\n",
    "- json                   : untuk menyimpan output ke dalam file .json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Inisiasi libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Membuat Fungsi Untuk Mendapatkan URL Berita dari Indeks\n",
    "---\n",
    "Sebelum melakukan ekstraksi data berita, terlebih dahulu dilakukan pengumpulan links dari halaman indeks pada website yang akan discrap. Nah, fungsi ini digunakan untuk mendapatkan semua url dari berita yang ada dalam halaman indeks tersebut. Untuk melakukan hal ini terlebih dahulu harus dipahami mengenai struktur dokumen HTML dari website tersebut, sehingga bisa diketahui mana _tag-tag_ yang perlu diambil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(start_date, end_date, category):       \n",
    "    links_berita = []\n",
    "    for i in range((end_date - start_date).days + 1):\n",
    "        \n",
    "        # membuat URL dari halaman indeks dengan format https://www.tempo.co/indeks/{tahun/{bulan}/{tanggal}/{kategori}     \n",
    "        tgl = start_date + i * timedelta(days=1)\n",
    "        tgl_str = datetime.strftime(tgl, '%Y/%m/%d')        \n",
    "        url = 'https://www.tempo.co/indeks/{}/{}'.format(tgl_str, category)      \n",
    "                \n",
    "        # mengambil halaman dari url dan menyimpannya dalam variabel string r\n",
    "        r = urllib.request.urlopen(url).read()     \n",
    "        \n",
    "        # ubah variabel r menjadi object BeautifulSoup\n",
    "        soup = BeautifulSoup(r,\"html.parser\")\n",
    "\n",
    "        # cari section khusus yang memuat daftar indeks berita \n",
    "        indeks_section = soup.find('section', class_=\"list list-type-1\")\n",
    "\n",
    "        # pecah section ke dalam daftar card berita\n",
    "        indeks_berita = indeks_section.find_all('div', class_=\"card card-type-1\")\n",
    "\n",
    "        # mendapatkan link untuk mengakses single page dari masing-masing berita \n",
    "        links_berita.extend([berita.a['href'] for berita in indeks_berita])\n",
    "\n",
    "    return links_berita                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Membuat Fungsi Untuk Membaca Berita\n",
    "\n",
    "Dari _link single page_ berita yang telah diperoleh, langkah berikutnya adalah mengekstrasi data dari halaman yang diperoleh dari _link_ tersebut. Fungsi ini dibuat untuk melakukan tugas tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBerita(link):\n",
    "    # memuat halaman web dari link yang diberikan dan menyimpan ke variabel html\n",
    "    html = urllib.request.urlopen(link).read()\n",
    "    \n",
    "    # mengkonversi variabel html menjadi object BeautifulSoup agar bisa diparsing\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # mencari bagian yang memuat artikel     \n",
    "    article = soup.find('article')\n",
    "    \n",
    "    # mengambil judul dari artikel     \n",
    "    judul = article.find(\"h1\", itemprop = \"headline\").text.strip().encode(\"utf8\").decode()\n",
    "    \n",
    "    # mengambil tanggal dari artikel     \n",
    "    tanggal = article.find(\"span\", itemprop = \"datePublished\").string\n",
    "    \n",
    "    # mengambil isi dari artikel, tetapi masih kasar     \n",
    "    isi_raw = article.find(\"div\", itemprop = \"articleBody\").find_all('p')\n",
    "    \n",
    "    # membersikan isi dari bagian-bagian yang tidak diperlukan\n",
    "    isi_clean = '\\n'.join([isi.text for isi in isi_raw if 'BACA:' not in isi.text and 'Simak Juga:' not in isi.text])        \n",
    "    \n",
    "    # mengambil url gambar dari artikel     \n",
    "    gambar = article.find(\"img\", itemprop = \"image\")['src']\n",
    "    \n",
    "    # mengambil nama redaksi dari artikel     \n",
    "    author = article.find(\"h4\", itemprop = \"author\").string\n",
    "    \n",
    "    # mengambil nama editor dari artikel         \n",
    "    editor = article.find(\"h4\", itemprop = \"editor\").string\n",
    "    \n",
    "    # mengambil keywords dari artikel         \n",
    "    tags = [t.string for t in article.find_all(\"a\", itemprop = \"keywords\")]       \n",
    "    \n",
    "    # simpan image berita ke folder images\n",
    "    urllib.request.urlretrieve(gambar, \"images\" + link[link.rindex('/'):]+gambar[gambar.rindex('.'):])\n",
    "   \n",
    "    # mengembalikan nilai dalam bentuk dict, agar mudah diolah menjadi JSON file\n",
    "    return {\"judul\": judul, \"isi\": isi_clean, \"tanggal\":tanggal, \"author\":author, \"editor\":editor, \"tags\":tags, \"gambar\": gambar }    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Mulai Scraping \n",
    "\n",
    "Proses _web scraping_ dilakukan pada tahap ini menggunakan dua fungsi yang telah dibuat sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping : https://tekno.tempo.co/read/1140023/ftui-buat-rumah-dual-power-pertama-ini-harganya\n",
      "Scraping Done.. \n"
     ]
    }
   ],
   "source": [
    "# inisiasi variabel untuk menampung hasil scraping\n",
    "berita = []\n",
    "\n",
    "# inisiasi parameter yang digunakan sebagai starting point proses scraping\n",
    "fromDate = datetime(2018,10,22)\n",
    "toDate   = datetime(2018,10,26)\n",
    "cat      = 'tekno'\n",
    "\n",
    "# mendaptkan seluruh links sesuai parameter seberlumnya\n",
    "links = get_links(fromDate, toDate, cat)\n",
    "\n",
    "# ekstraksi data dari setiap link yang telah didapat dan menyimpanya ke variabel berita[]\n",
    "for link in links:\n",
    "    clear_output()\n",
    "    berita.append(getBerita(link))\n",
    "    print('scraping :', link)\n",
    "      \n",
    "\n",
    "# menyimpan hasil scraping ke dalam file\n",
    "with open('tempo_' + cat + '_'+ datetime.strftime(fromDate, '%Y-%m-%d') +'_to_' + datetime.strftime(toDate, '%Y-%m-%d') +'.json', 'w') as outfile:  \n",
    "    json.dump(berita, outfile)\n",
    "    print('Scraping Done.. ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(berita)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
